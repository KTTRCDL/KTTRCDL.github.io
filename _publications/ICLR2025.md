---
title: "Let Your Features Tell The Differences: Understanding Graph Convolution By Feature Splitting"
collection: publications
category: conferences
permalink: /publication/ICLR2025
excerpt: 'This paper proposes a new metric to identify GNN-favored and GNN-disfavored features and use topological feature selection to fuse these features into GNNs, which significantly improves GNNs performance without hyper-parameter tuning.'
date: 2025-01-23
venue: 'ICLR'
paperurl: 'https://openreview.net/pdf?id=I9omfcWfMp'
citation: 'Yilun Zheng, Xiang Li, Sitao Luan, Xiaojiang Peng, and Lihui Chen. (2025). &quot;Let your features tell the differences: Understanding graph convolution by feature splitting.&quot; <i>The Thirteenth International Conference on Learning Representations</i>.'
---

Graph Neural Networks (GNNs) have demonstrated strong capabilities in processing structured data. While traditional GNNs typically treat each feature dimension
equally important during graph convolution, we raise an important question: Is
the graph convolution operation equally beneficial for each feature? If not, the
convolution operation on certain feature dimensions can possibly lead to harmful effects, even worse than convolution-free models. Therefore, it is required to
distinguish convolution-favored and convolution-disfavored features. Traditional
feature selection methods mainly focus on identifying informative features or reducing redundancy, but they are not suitable for structured data as they overlook
graph structures. In graph community, some studies have investigated the performance of GNN with respect to node features using feature homophily metrics, which assess feature consistency across graph topology. Unfortunately, these
metrics do not effectively align with GNN performance and cannot be reliably
used for feature selection in GNNs. To address these limitations, we introduce
a novel metric, Topological Feature Informativeness (TFI), to distinguish GNNfavored and GNN-disfavored features, where its effectiveness is validated through
both theoretical analysis and empirical observations. Based on TFI, we propose
a simple yet effective Graph Feature Selection (GFS) method, which processes
GNN-favored and GNN-disfavored features with GNNs and non-GNN models
separately. Compared to original GNNs, GFS significantly improves the extraction of useful topological information from each feature with comparable computational costs. Extensive experiments show that after applying GFS to 8 baseline and state-of-the-art (SOTA) GNN architectures across 10 datasets, 90% of
the GFS-augmented cases show significant performance boosts. Furthermore, our
proposed TFI metric outperforms other feature selection methods for GFS. These
results verify the effectiveness of both GFS and TFI. Additionally, we demonstrate that GFSâ€™s improvements are robust to hyperparameter tuning, highlighting
its potential as a universally valid method for enhancing various GNN architectures. To facilitate reproducibility and further research, we have made our code
publicly available at [graph-feature-selection](https://github.com/KTTRCDL/graph-feature-selection).

Please refer to the [paper](https://openreview.net/pdf?id=I9omfcWfMp) for more details. If citation is needed, please use the following BibTeX entry:

```bibtex
@inproceedings{zheng2025let,
  title={Let your features tell the differences: Understanding graph convolution by feature splitting},
  author={Zheng, Yilun and Li, Xiang and Luan, Sitao and Peng, Xiaojiang and Chen, Lihui},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}
```
